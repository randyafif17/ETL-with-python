{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\randy\\anaconda3\\envs\\ai\\lib\\site-packages (19.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\randy\\appdata\\roaming\\python\\python311\\site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\randy\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement random (from versions: none)\n",
      "ERROR: No matching distribution found for random\n"
     ]
    }
   ],
   "source": [
    "!pip install faker\n",
    "!pip install random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"passwd\": \"YOURPASSWORD\",\n",
    "    \"database\": \"new\"\n",
    "}\n",
    "\n",
    "# Initialize Faker for generating fake data\n",
    "fake = Faker()\n",
    "\n",
    "num_records = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\AppData\\Local\\Temp\\ipykernel_12252\\3711961086.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_mysql = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL process completed and data loaded to MySQL database successfully! Combined data is saved to combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate fake data\n",
    "def generate_fake_data(num_records):\n",
    "    data = {\n",
    "        'name': [fake.name() for _ in range(num_records)],\n",
    "        'email': [fake.email() for _ in range(num_records)],\n",
    "        'address': [fake.address() for _ in range(num_records)],\n",
    "        'purchase_amount': [round(fake.random_number(digits=2) + fake.random_digit()/100, 2) for _ in range(num_records)],\n",
    "        'purchase_date': [fake.date_between(start_date='-1y', end_date='today').isoformat() for _ in range(num_records)]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract data from MySQL\n",
    "def extract_from_mysql(db_params, table_name):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_params)\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df_mysql = pd.read_sql(query, connection)\n",
    "        return df_mysql\n",
    "    except Error as error:\n",
    "        print(f\"Error: {error}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "# Transform phase (clean and format data)\n",
    "def transform(dataframe):\n",
    "    dataframe['email'] = dataframe['email'].str.lower()\n",
    "    dataframe['purchase_date'] = pd.to_datetime(dataframe['purchase_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Load data to MySQL\n",
    "def load_to_mysql(dataframe, table_name, db_params):\n",
    "    engine_str = f\"mysql+pymysql://{db_params['user']}:{db_params['passwd']}@{db_params['host']}/{db_params['database']}\"\n",
    "    engine = create_engine(engine_str)\n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "# ETL process function\n",
    "def etl_process():\n",
    "    # Extract\n",
    "    df_mysql = extract_from_mysql(db_params, 'data_from_sql')\n",
    "    \n",
    "    # Generate fake data\n",
    "    df_fake = generate_fake_data(num_records)  # Generate fake records\n",
    "    \n",
    "    # Get the last id from the MySQL extracted data\n",
    "    last_id = df_mysql['id'].max()\n",
    "\n",
    "    # Assign new incremental ids for the fake data\n",
    "    df_fake['id'] = range(last_id + 1, last_id + 1 + len(df_fake))\n",
    "\n",
    "    # Transform\n",
    "    df_mysql = transform(df_mysql)\n",
    "    df_fake = transform(df_fake)\n",
    "    \n",
    "    # Combine data\n",
    "    df_combined = pd.concat([df_mysql, df_fake], ignore_index=True)\n",
    "    \n",
    "    # Reorder the columns to have 'id' first\n",
    "    df_combined = df_combined[['id', 'name', 'email', 'address', 'purchase_amount', 'purchase_date']]\n",
    "    \n",
    "    # Load to MySQL\n",
    "    load_to_mysql(df_combined, 'new_table', db_params)\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    output_file = 'combined_data.csv'\n",
    "    df_combined.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"ETL process completed and data loaded to MySQL database successfully! Combined data is saved to {output_file}\")\n",
    "\n",
    "# Execute the ETL process\n",
    "etl_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional transformations applied and saved to CSV file.\n",
      "    id                   name                              email  \\\n",
      "0    1               john doe                                NaN   \n",
      "1    2             Jane Smith             jane.smith@example.com   \n",
      "2    3          alice johnson          alice.johnson@example.com   \n",
      "3    4          michael brown          michael.brown@example.com   \n",
      "4    5            Emily Davis            emily.davis@example.com   \n",
      "5    6          daniel garcia          DANIEL.GARCIA@EXAMPLE.COM   \n",
      "6    7           Laura Wilson           LAURA.WILSON@EXAMPLE.COM   \n",
      "7    8         Kevin Martinez                                NaN   \n",
      "8    9      Isabella Anderson      isabella.anderson@example.com   \n",
      "9   10          Joshua Thomas          joshua.thomas@example.com   \n",
      "10  11             sophia lee             sophia.lee@example.com   \n",
      "11  12  christopher hernandez  christopher.hernandez@example.com   \n",
      "12  13           Jessica King                                NaN   \n",
      "13  14         Matthew Wright         matthew.wright@example.com   \n",
      "14  15            olivia hill            olivia.hill@example.com   \n",
      "15  16            ethan scott            ethan.scott@example.com   \n",
      "16  17         Madison Torres         madison.torres@example.com   \n",
      "17  18       Alexander Nguyen       alexander.nguyen@example.com   \n",
      "18  19          abigail young          abigail.young@example.com   \n",
      "19  20            David Adams                                NaN   \n",
      "20   3          alice johnson          alice.johnson@example.com   \n",
      "21  19          abigail young          abigail.young@example.com   \n",
      "\n",
      "                                              address purchase_amount  \\\n",
      "0                                                 NaN             NaN   \n",
      "1              0467 Jeff Flats\\nJohnsontown, ME 89324           55.09   \n",
      "2      29606 Kelly Wall Apt. 548\\nLake Lisa, AK 89418             NaN   \n",
      "3   426 Hicks Mall Apt. 164\\nNorth Christophervill...           17.92   \n",
      "4                                                 NaN             NaN   \n",
      "5                                                 NaN             NaN   \n",
      "6   158 Anlerson Bridge Apt. 097\\nNew Kathleen, OK...           80.32   \n",
      "7                                                 NaN           13.64   \n",
      "8         91524 Kelly Spring\\nJenningsville, WY 43311           93.89   \n",
      "9                    Unit 5861 Box 7950\\nDPO AP 11695           13.76   \n",
      "10             491 Hardin Port\\nWest Angela, TN 93468           67.06   \n",
      "11   312 Bell Plain Apt. 884\\nNorth Zachary, PR 29492           57.36   \n",
      "12           141 Lori Mills\\nPort Juliebury, VI 10390           37.07   \n",
      "13  34543 Janet Knolls Suite 001\\nNew Michaelbury,...           28.09   \n",
      "14   197 Joseph Corners Suite 792\\nSotobury, GA 94128           16.66   \n",
      "15  181 Matthew Junctions Suite 586\\nLake Gregory,...           11.91   \n",
      "16   629 Lacey Glen\\nWest Kimberlyton, AR 81100 75937           93.97   \n",
      "17  189 Erin Passage Suite 849\\nLewischester, PW 7...           30.44   \n",
      "18                         USCGC Taylor\\nFPO AP 47892            38.3   \n",
      "19       74635 Pham Roads\\nLake Carolynside, RI 19985           75.48   \n",
      "20     29606 Kelly Wall Apt. 548\\nLake Lisa, AK 89418           90.16   \n",
      "21                         USCGC Taylor\\nFPO AP 47892            38.3   \n",
      "\n",
      "   purchase_date  \n",
      "0     2023/03/03  \n",
      "1     2023/02/04  \n",
      "2     2023/01/09  \n",
      "3     2023-10-01  \n",
      "4     2023-02-05  \n",
      "5     2023-05-12  \n",
      "6     2023-05-30  \n",
      "7     2023/06/25  \n",
      "8     2023/07/04  \n",
      "9     2023-01-14  \n",
      "10    2023-06-14  \n",
      "11    2023/07/25  \n",
      "12    2023-08-06  \n",
      "13    2023/09/28  \n",
      "14    2023-05-19  \n",
      "15    2023-04-21  \n",
      "16    2023-09-29  \n",
      "17    2023/01/15  \n",
      "18    2022/12/25  \n",
      "19    2023-08-19  \n",
      "20    2023/01/09  \n",
      "21    2022/12/25  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Additional transformations on the DataFrame\n",
    "def additional_transformations(dataframe):\n",
    "    # Randomly set 10% of email column to NaN\n",
    "    dataframe.loc[np.random.choice(dataframe.index, size=int(0.1 * len(dataframe)), replace=False), 'email'] = np.nan\n",
    "\n",
    "    # Change format for 50% of purchase_date from YYYY-MM-DD to YYYY/MM/DD\n",
    "    dataframe.loc[dataframe.sample(frac=0.50).index, 'purchase_date'] = \\\n",
    "        dataframe['purchase_date'].apply(lambda x: x.replace('-', '/'))\n",
    "\n",
    "    # Convert names to lowercase for 5% of the records\n",
    "    dataframe.loc[dataframe.sample(frac=0.50).index, 'name'] = \\\n",
    "        dataframe['name'].str.lower()\n",
    "\n",
    "    # Convert emails to uppercase for 5% of the records\n",
    "    dataframe.loc[dataframe.sample(frac=0.05).index, 'email'] = \\\n",
    "        dataframe['email'].str.upper()\n",
    "\n",
    "    # Introduce duplicates for 5% of the records\n",
    "    duplicates = dataframe.sample(frac=0.05)\n",
    "    dataframe = pd.concat([dataframe, duplicates], ignore_index=True)\n",
    "\n",
    "    # Increase purchase_amount by 1000 for 1% of the records\n",
    "    dataframe.loc[dataframe.sample(frac=0.01).index, 'purchase_amount'] += 1000\n",
    "\n",
    "    # Set purchase_amount to negative for 1% of the records\n",
    "    dataframe.loc[dataframe.sample(frac=0.01).index, 'purchase_amount'] *= -1\n",
    "\n",
    "    # Set purchase_amount to 'unknown' for 1% of the records\n",
    "    indices_to_change = dataframe.sample(frac=0.01).index\n",
    "    dataframe.loc[indices_to_change, 'purchase_amount'] = 'unknown'\n",
    "    # Ensure that the purchase_amount column is of object type if 'unknown' values are introduced\n",
    "    dataframe['purchase_amount'] = dataframe['purchase_amount'].astype(object)\n",
    "\n",
    "    na_indices = np.random.choice(dataframe.index, size=int(0.1 * len(dataframe)), replace=False)\n",
    "    dataframe.loc[na_indices, 'purchase_amount'] = np.nan\n",
    "\n",
    "    dataframe.loc[np.random.choice(dataframe.index, size=int(0.1 * len(dataframe)), replace=False), 'address'] = np.nan\n",
    "\n",
    "    # Introduce typos for 5% of the address records\n",
    "    def introduce_typos(text):\n",
    "        if pd.isnull(text) or len(text) < 5:  # Jika NaN atau terlalu pendek, tidak berubah\n",
    "            return text\n",
    "        char_pos = random.randint(0, len(text) - 1)\n",
    "        return text[:char_pos] + random.choice('abcdefghijklmnopqrstuvwxyz') + text[char_pos + 1:]\n",
    "    \n",
    "    typo_indices = dataframe.sample(frac=0.05).index\n",
    "    dataframe.loc[typo_indices, 'address'] = dataframe.loc[typo_indices, 'address'].apply(introduce_typos)\n",
    "\n",
    "    # Concatenating address information from other records to create inconsistencies\n",
    "    # For a small fraction of records, combine address details from other records\n",
    "    concat_indices = dataframe.sample(frac=0.05).index\n",
    "    for idx in concat_indices:\n",
    "        if len(dataframe) > idx + 1:  # Avoid the last index\n",
    "            dataframe.at[idx, 'address'] += ' ' + dataframe.at[idx + 1, 'address'].split(' ')[-1]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "dataframe = pd.read_csv('raw_data.csv')\n",
    "# Assuming df is the DataFrame after the ETL process\n",
    "# Apply the additional transformations\n",
    "df = additional_transformations(dataframe)\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file\n",
    "df.to_csv('combined_data.csv', index=False)\n",
    "load_to_mysql(df, 'new_table', db_params)\n",
    "\n",
    "\n",
    "print(\"Additional transformations applied and saved to CSV file.\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def clean_data(dataframe):\n",
    "    # Fill missing emails with a placeholder\n",
    "    dataframe['email'].fillna('no_email_provided', inplace=True)\n",
    "    \n",
    "    # Introduce a placeholder for missing addresses\n",
    "    dataframe['address'].fillna('No Address Provided', inplace=True)\n",
    "    \n",
    "    # Handle missing or 'unknown' purchase_amount\n",
    "    # Convert 'unknown' to NaN and then fill NaN with the mean of the column\n",
    "    dataframe['purchase_amount'] = pd.to_numeric(dataframe['purchase_amount'], errors='coerce')\n",
    "    dataframe['purchase_amount'].fillna(dataframe['purchase_amount'].mean(), inplace=True)\n",
    "    # dataframe['purchase_amount'].fillna(dataframe['purchase_amount'].median(), inplace=True) #menggunakan nilai median\n",
    "    # dataframe['purchase_amount'].fillna(dataframe['purchase_amount'].mode()[0], inplace=True) #menggunakan modus\n",
    "    # dataframe['purchase_amount'].fillna(0, inplace=True) #menggunakan nilai tetap\n",
    "    # dataframe['purchase_amount'].fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    # dataframe['purchase_amount'].fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # dataframe['purchase_amount'].interpolate(method='linear', inplace=True) #interpolasi\n",
    "    \n",
    "    # Correct the date format for purchase_date\n",
    "    dataframe['purchase_date'] = dataframe['purchase_date'].apply(lambda x: x.replace('/', '-'))\n",
    "    \n",
    "    # Normalize the case of names and emails\n",
    "    dataframe['name'] = dataframe['name'].str.title()\n",
    "    dataframe['email'] = dataframe['email'].str.lower()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Reset the index after removing duplicates\n",
    "    dataframe.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Misalkan df adalah DataFrame setelah proses transformasi tambahan\n",
    "# Membersihkan data dengan fungsi clean_data\n",
    "df_cleaned = clean_data(df)\n",
    "\n",
    "# Simpan DataFrame yang telah dibersihkan ke file CSV baru\n",
    "df_cleaned.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "# Jika Anda ingin memuat data yang telah dibersihkan kembali ke MySQL:\n",
    "load_to_mysql(df_cleaned, 'clean_table', db_params)\n",
    "\n",
    "print(\"Data cleaned and saved to CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
